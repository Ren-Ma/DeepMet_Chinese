"""用wobert预测隐喻词汇的平替词
Env: 2026, wordreplace"""
import os
import re
import jieba
import pandas as pd
import numpy as np
from tqdm import tqdm
import ast
os.getcwd()
from bert4keras.models import build_transformer_model
from bert4keras.tokenizers import Tokenizer
from bert4keras.snippets import to_array

from utils_DeepMet import flattern_lst_of_lst, keep_only_CN_char

def word_encode(word):
    """encode a single word to token_ids, without CLS or SEP"""
    token_ids, _ = tokenizer.encode(word)  # encode每个词
    token_ids = token_ids[1: -1]  # 去掉前缀CLS和后缀SEP
    return token_ids

def sentence_encode(sentence):
    sentence_tokenized = jieba.lcut(sentence)
    sentence_token_ids = [word_encode(word) for word in sentence_tokenized]
    return sentence_token_ids

def split_sentence(sentence, split_word):
    """按照split_word把sentence分割开，输出为list"""
    split_word_re = '(' + split_word + ')'  # 把re表达式放进括号内，表示在输出中保留分割项。
    sentence_split = re.split(split_word_re, sentence)
    sentence_split = [x for x in sentence_split if x != '']
    return sentence_split

def predict_replaced_word(list_of_token_ids):
    """根据token_ids来预测mask掉的词，输出前100个候选词"""
    list_of_token_ids = flattern_lst_of_lst(list_of_token_ids)
    # add CLS and SEP tokens before and after. NOTE!!! the '[]' out of list_of_token_ids is required
    token_ids = to_array([[101] + list_of_token_ids + [102]])
    segment_ids = np.zeros_like(token_ids)
    # NOTE!!! the [0] at the end
    probas = model.predict([token_ids, segment_ids])[0]
    # NOTE!!! the [0] after token_ids, and after *103]
    this_replaced_word = [tokenizer._token_dict_inv[x] for x in probas[token_ids[0] == 103][0].argsort()[-100:][::-1]]
    return keep_only_CN_char(this_replaced_word)


def predict_wobert_one_row(row, sentence_col_name, meta_word_col):
    """基于wobert预测隐喻词汇的平替词"""
    sentence_split = split_sentence(row[sentence_col_name], (row[meta_word_col]))
    meta_words_ids = [word_encode(row[meta_word_col])]  # 获取meta_words的token_ids

    # [[sent1_word1_ids, sent1_word2_ids, ...], [sent2_word1_ids, sent2_word2_ids, ...], ...]
    sentence_token_ids = [sentence_encode(sent) for sent in sentence_split]
    # [sent1_word1_ids, sent1_word2_ids, ..., sent2_word1_ids, sent2_word2_ids, ...]
    sentence_token_ids = flattern_lst_of_lst(sentence_token_ids)

    sentence_token_ids_masked = [ids if ids not in meta_words_ids else [tokenizer._token_mask_id] for ids in
                                 sentence_token_ids]
    predicted_meta_words_wobert = predict_replaced_word(sentence_token_ids_masked)
    return predicted_meta_words_wobert


if __name__ == '__main__':
    # 读取文件
    # df_expand = pd.read_csv('./corpus/CCL_PSUCMC_meta_bert.csv', converters={"meta_words": ast.literal_eval})
    # df = pd.read_excel('./corpus/poetry/poetry0-100007w_predict薛24867_edited_top1000.xlsx')
    filename = 'poetry0-24859w_predict薛24867_edited'
    df = pd.read_excel('./corpus/poetry/' + filename + '.xlsx')
    meta_word_col = 'meta_words'  # 找到隐喻词汇所在列的名称
    sentence_col = 'content'  # 找到完整句子所在列的名称
    df[meta_word_col] = df[meta_word_col].fillna('')
    df[meta_word_col] = df[meta_word_col].apply(lambda x: list(set(x.split('、'))))  # 分割隐喻词并去除重复词
    # df[meta_word_col] = df[meta_word_col].apply(lambda x: list(set(x)))
    # df_expand = df.explode(meta_word_col, ignore_index=True)  # 把meta_word_col这一列展开，其他列重复
    df_expand = df.explode(meta_word_col)
    jieba.load_userdict("./corpus/mydict/mydict0408.txt")  # 每次都要用到最新的mydict
    # 启动模型
    config_path = './chinese_wobert_plus_L-12_H-768_A-12/bert_config.json'
    checkpoint_path = './chinese_wobert_plus_L-12_H-768_A-12/bert_model.ckpt'
    dict_path = './chinese_wobert_plus_L-12_H-768_A-12/vocab.txt'
    tokenizer = Tokenizer(dict_path, do_lower_case=True)  # 建立分词器

    df_expand['meta_sub_wobert'] = ''
    # 采用apply功能，只输出df中的隐喻句子
    typo_filter = [(row[meta_word_col] == '') or (row[meta_word_col] in row[sentence_col])
            for idx, row in df_expand.iterrows()]
    idxs = [idx for idx, row in df_expand.iterrows() if
            (row[meta_word_col] != '') and (row[meta_word_col] not in row[sentence_col])]
    df_expand_notypo = df_expand[typo_filter]
    meta_filter = [row[meta_word_col] != '' for
                   idx, row in df_expand_notypo.iterrows()]
    # df_expand_onlymeta = df_expand[df[meta_word_col] != ['']].apply(predict_wobert_one_row, axis=1)
    # 采用loop迭代，输出整个df
    model = build_transformer_model(config_path=config_path, checkpoint_path=checkpoint_path,
                                    with_mlm=True)  # 建立模型，加载权重
    df_expand_notypo.reset_index(drop=True, inplace=True)
    df_expand_copy = df_expand_notypo.copy()
    for idx, row in tqdm(df_expand_notypo.iterrows()):
        if row[meta_word_col] != '':
            df_expand_copy.at[idx, 'meta_sub_wobert'] = predict_wobert_one_row(row, sentence_col, meta_word_col)

    df_expand_meta = df_expand_copy[meta_filter]
    df_expand_copy.to_excel('./corpus/poetry/' + filename + '_wobert.xlsx', index=False)
    df_expand_meta.to_excel('./corpus/poetry/' + filename + '_meta_wobert.xlsx', index=False)



